{
  
    
        "post0": {
            "title": "Tokenization for language modeling: Byte Pair Encoding vs Unigram Language Modeling",
            "content": "My thanks to Chris Potts and to Eric Jeske for valuable discussion of the ideas here and for their comments on early drafts of this post. . Imagine that you&#39;re learning English and you come across the word destabilizing. You&#39;ve never seen it before, but you have seen de- as a prefix in lots of other words and know it&#39;s something to do with negating what follows, and you&#39;ve seen stabilizing too. With that information, you can come to a more-or-less perfect understanding of destabilizing. . Now imagine that you mis-parsed the word and read it as dest-abilizing. Well, you&#39;ve seen dest as the beginning of destination, destiny and destroy—perhaps dest is something to do with a future state? You&#39;ve seen ize and ing suffixes, guess that abilizing means something like enabling, and conclude that destabilizing means something like becoming able to do something. . Unfortunately, common language tokenizers (including those used in Bert and GPT-2) misread words in just this way all the time: . from transformers import BertTokenizer, GPT2Tokenizer from helpers.tokenization import * bert_tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-cased&#39;) gpt2_tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;) tokenize_funcs = [ (&#39;Bert&#39;, bert_tokenizer), (&#39;GPT-2&#39;, gpt2_tokenizer), ] FormattedTable(tokenize_funcs).print_table_from_words(&quot;stabilizing&quot;, &quot;destabilizing&quot;) . . stabilizing destabilizing Bert stab-ili-zing des-ta-bil-izing GPT-2 stabil-izing destabil-izing . There&#39;s no overlap between Bert&#39;s tokenizations of stabilizing and destabilizing: the model must learn about these words completely independently. GPT-2&#39;s tokenizer at least shares izing so the model has an indication that the words have something in common (albeit with capsizing too): . FormattedTable(tokenize_funcs).print_table_from_words(&quot;capsizing&quot;) . . capsizing Bert caps-izing GPT-2 caps-izing . Now consider stigmatize and destigmatize: not only is the significance of de missed, but both tokenizers give the model the misleading information that there is a relationship between destigmatize and destinies, even though the de is operating as a negator in the first, and part of the morpheme des in the second: . words = [&#39;stigmatize&#39;, &#39;destigmatize&#39;, &#39;destinies&#39;] FormattedTable(tokenize_funcs).print_table_from_words(*words) . . stigmatize destigmatize destinies Bert s-ti-gma-ti-ze des-ti-gma-ti-ze des-tin-ies GPT-2 stigmat-ize dest-igmat-ize dest-in-ies . A human reading dest-igmat-ize would have trouble understanding it until they recognized that the st belongs in the next group. Indeed, hyphenation dictionaries for e-readers disallow hyphens that break syllables for exactly this reason. . Byte pair encoding (BPE) . The tokenizer used by GPT-2 (and most variants of Bert) is built using byte pair encoding (BPE). Bert itself uses some proprietary heuristics to learn its vocabulary but uses the same greedy algorithm as BPE to tokenize. BPE comes from information theory: the objective is to maximally compress a dataset by replacing common substrings with tokens, morphology be damned. . Are we disadvantaging language models by partitioning words in ways that obscure the relationships between them? Perhaps this doesn&#39;t matter. Traditional NLP models used whole words as tokens which by definition have to be learned independently, so we&#39;re better off than we used to be! Bert and GPT-2 are able to achieve astounding results with these &#39;misleading&#39; tokens. The models are large enough that (presumably) the dest subword representation gets a little bit of de- plus a little bit of destiny/destroy/destination/etc, and they train on so much data that they see variants of stabilize often enough to learn the same aspects of &#39;meaning&#39; redundantly. . On the other hand, the job of machine learning algorithms is to find and learn regularities in the training data. Providing them with the tokens that best capture these regularities (such as prefixes and suffixes) seems likely to help. Perhaps such a model could be smaller or could train more quickly. . Oddly, though, BPE has been more-or-less universally accepted as the standard preprocessing step for language modelling. Even Google&#39;s T5 paper which, in 30 meticulous pages, adjusted and evaluated every hyperparameter imaginable⁠, treated the tokenizer as fixed. The RoBERTa paper does consider tokenization in the Text Encoding section, but only compares variants of BPE (Bert&#39;s tokenizer and vanilla BPE). . Unigram language modeling . Recent work by Kaj Bostrom and Greg Durrett showed that by simply replacing BPE with a different method, morphology is better preserved and a language model trained on the resulting tokens shows improvements when fine tuned on downstream tasks. . Surprisingly, the replacement—unigram language modeling—has been under our noses all along. (Don&#39;t be confused with the language model in the name: this is still the preprocessing step.) Unigram LM tokenizers are already implemented in Google&#39;s sentencepiece library, but it was introduced in a paper ostensibly about data augmentation for translation models and so the wider significance seems to have been missed. . Bostrom and Durrett&#39;s paper show some examples illustrating the difference: . . They explain that unigram LMs recover common suffixes like &#39;ly&#39;, &#39;s&#39; and &#39;ing&#39; much more often than BPE. This is exactly what we wanted! I trained my own unigram LM and BPE tokenizers on 10 million randomly-selected sentences from Wikipedia to see if I could replicate these differences and to compare how they treat my destabilizing example. Here are the results with vocabularies limited to 8k and 32k words: . my_tokenizers = [ (&#39;BPE 8k&#39;, load_tokenizer(&#39;bpe&#39;, 8)), (&#39;BPE 32k&#39;, load_tokenizer(&#39;bpe&#39;, 32)), (&#39;Unigram LM 8k&#39;, load_tokenizer(&#39;ulm&#39;, 8)), (&#39;Unigram LM 32k&#39;, load_tokenizer(&#39;ulm&#39;, 32)), ] FormattedTable(my_tokenizers).print_table_from_words(&quot;stabilizing&quot;, &quot;destabilizing&quot;) . . stabilizing destabilizing BPE 8k st-ab-il-izing dest-ab-il-izing BPE 32k stabil-izing dest-abil-izing Unigram LM 8k sta-bil-izing de-sta-bil-izing Unigram LM 32k stabiliz-ing de-stabiliz-ing . This seems to have worked well! Both Unigram LM tokenizers treat destabilizing as de+stabilizing, but BPE doesn&#39;t. (It&#39;s not quite this simple: I&#39;ve hidden the fact that the first token in a word is necessarily distinct from any subsequent token to keep track of word boundaries; more on that later.) . Evaluating tokenizers . Bostrom and Durrett&#39;s paper includes comparisons of the length distribution of tokens and the frequency with which they are used, but that doesn&#39;t reflect how a tokenizer captures morphology. To do that, we need gold standard tokenizations: the ones that best capture pronunciation while preserving spelling. Luckily, that&#39;s exactly what Merriam Webster provides, at least for headwords (i.e. the base form of a word from which others derive: stabilize is the headword of stabilizing). For example, here&#39;s its entry for destabilize with the pronunciation guide highlighted: . . I&#39;ll call a subword morphologically sound if it consists of one or more entire Merriam Webster subwords. For example, de, lize, stabilize and destabilize are all morphologically sound, but dest isn&#39;t. We can&#39;t expect a tokenizer to choose &#39;morphologically sound&#39; subwords every time since that would require an enormous vocabulary; nevertheless, if tokenizer A produces more morphologically sound subwords than tokenizer B, it seems reasonable to conclude that tokenizer A better captures morphology than B. The table below shows a few examples color-coded to indicate which subwords are morphologically sound: . tokenizers = [ (&#39;BPE 32k&#39;, load_tokenizer(&#39;bpe&#39;, 32)), (&#39;Unigram LM 32k&#39;, load_tokenizer(&#39;ulm&#39;, 32)), (&#39;Bert&#39;, bert_tokenizer), (&#39;GPT-2&#39;, gpt2_tokenizer), ] examples_to_display = [ [&quot;spac&quot;, &quot;ing&quot;], [&quot;pen&quot;, &quot;chant&quot;], [&quot;brachy&quot;, &quot;ther&quot;, &quot;a&quot;, &quot;py&quot;], [&quot;in&quot;, &quot;ter&quot;, &quot;mo&quot;, &quot;lec&quot;, &quot;u&quot;, &quot;lar&quot;], [&quot;blurt&quot;], [&quot;whit&quot;, &quot;en&quot;], [&quot;nod&quot;, &quot;u&quot;, &quot;lar&quot;], [&#39;daunt&#39;, &#39;ing&#39;], ] FormattedTable(tokenizers).print_table_from_gold_standard(*examples_to_display) . . spacing penchant Gold standard spac-ing pen-chant BPE 32k sp-acing pen-chant Unigram LM 32k spacing pen-chant Bert spa-cing pen-chan-t GPT-2 spacing penchant brachytherapy intermolecular Gold standard brachy-ther-a-py in-ter-mo-lec-u-lar BPE 32k bra-chy-the-rapy inter-m-ole-cular Unigram LM 32k bra-chy-therapy inter-molecular Bert bra-chy-ther-ap-y inter-mo-le-cular GPT-2 br-achy-ther-apy inter-m-ole-cular blurt whiten Gold standard blurt whit-en BPE 32k bl-urt whit-en Unigram LM 32k blur-t white-n Bert blur-t white-n GPT-2 bl-urt whit-en nodular daunting Gold standard nod-u-lar daunt-ing BPE 32k nod-ular da-unting Unigram LM 32k no-d-ular d-aunt-ing Bert nod-ular da-unting GPT-2 nod-ular daunting . I score how well each tokenizer does on this task with the fraction of the subwords produced by a tokenizer that are morphologically sound (i.e. green). Bert gets a score of 6/22 = 27% for the sample above because 6 of its 22 subwords are green (pen, ther, inter, mo, nod, and ular), while GPT-2 gets 10/17 = 59% because 10 of its 17 subwords are green (including whole-word subwords like spacing and penchant). . We can&#39;t draw conclusions from this tiny sample, so I repeated the evaluation with a much larger sample of 8,000 words. Since having access to more subwords allows a tokenizer to make fewer splits and thus provides an advantage, I built tokenizers with different vocabulary sizes and plotted their scores against the vocabulary size (not counting tokens that include any non-ASCII characters to control for different numbers of characters represented). . plot_tokenizer_evaluation_scores() . It&#39;s clear that Unigram LM-based tokenizers beat the BPE-based ones by a substantial margin. This supports Bostrom and Durrett&#39;s claim that &quot;the unigram LM method tends to produce more tokens that are morphologically interpretable.&quot; . Speed . We might worry that this comes at the cost of slower performance. But we needn&#39;t be concerned: while learning a Unigram LM tokenizer takes longer than BPE, it still took less than an hour for 10 million sentences on a 4-core, 32GB RAM EC2 machine (r5ad.xlarge): . plot_learning_speed() . Interestingly, the Unigram LM gets faster to learn as the vocab size increases. That&#39;s because the algorithm starts with a larger set and prunes vocab items until the desired vocab size is reached, while BPE initializes with an empty vocabulary and adds tokens. . For inference, the difference is insignificant (the shaded area and vertical bars show 95% confidence intervals estimated by repeating the experiments 10 times): . plot_inference_speed() . Bert and GPT-2&#39;s tokenizers are evaluated here using the transformers library which probably explains the slower performance compared to sentencepiece. . What&#39;s next? . In the short term, I share Bostrom and Durret&#39;s hope &quot;that developers of future pretrained language models will consider adopting the unigram LM method over the more common BPE.&quot; But I suspect that there remain opportunities for further improvements. . First, as mentioned above, all tokenizers in common use treat subwords at the start of a word differently from those inside words. This is to permit unambiguous reconstruction of strings from token sequences (originally discussed in Neural Machine Translation of Rare Words with Subword Units). There may be other ways to achieve that aim without doubling the vocabulary, for instance by adding a new_word mask as an additional input dimension (which would be predicted by a seq-to-seq model as a secondary output). . Second, it&#39;s not clear to me using compression algorithms to preprocess inputs is a good idea at all. The renaissance of deep learning came after it was discovered that, with an appropriate architecture and sufficient compute, deep learning methods could learn better feature representations for images than humans could invent. That architecture—the convolutional neural network—embeds the nature of two-dimensional space and an invariance that objects can appear in different parts of an image. . The equivalent approach for natural language processing would have us treat raw characters (or bytes) as input. Character-level language modeling isn&#39;t novel: Andrej Karpathy&#39;s influential blog post The Unreasonable Effectiveness of Recurrent Neural Networks showed—in 2015—that character-level LSTMs could generate realistic text. Google&#39;s 2018 paper Character-Level Language Modeling with Deeper Self-Attention showed an impressive ability to memorize and reproduce random character sequences appearing in natural text, but it did it with sequences of at most 512 characters: less than two tweets, and much shorter than sequences used in the leading language models. . This length limitation comes from the attention mechanism used in transformers which scales quadratically in the length of the sequence. Replacing a token sequence with an equivalent character sequence would be computationally intractable. Linguists often represent text sequences as trees, and this might provide a clue as to how we can reduce computation while embedding more of language&#39;s structure in model architectures. . . _Example of a parse tree from Wikipedia._ . Attention can model these trees since every token can directly attend to any other, unlike RNNs which model distances linearly. That is, attention gives ball a direct connection to John. But it&#39;s rare that a token should need to attend to another 500 tokens away, and so most of this computation is wasted. To best reflect a tree structure, we&#39;d want attention to be applied to windowed subsequences at the bottom layers of a network and for those layers to produce a shorter representation that can be combined via attention with the outputs from neighboring windows. That would allow the bottom layer to operate on characters while remaining computationally tractable. I have vague ideas about how this might be realized, but I&#39;m poorly-equipped to test them out! I&#39;d love to hear about attempts to do anything like this, or about why it wouldn&#39;t work. You can tweet me @ndingwall or comment below. .",
            "url": "https://ndingwall.github.io/personal-blog/tokenization",
            "relUrl": "/tokenization",
            "date": " • Jul 9, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ndingwall.github.io/personal-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ndingwall.github.io/personal-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ndingwall.github.io/personal-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}