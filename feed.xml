<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://ndingwall.github.io/personal-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ndingwall.github.io/personal-blog/" rel="alternate" type="text/html" /><updated>2022-06-08T23:37:36-05:00</updated><id>https://ndingwall.github.io/personal-blog/feed.xml</id><title type="html">Nick Dingwall</title><subtitle>Personal blog. Opinions and mistakes my own.</subtitle><entry><title type="html">Tokenization for language modeling: Byte Pair Encoding vs Unigram Language Modeling</title><link href="https://ndingwall.github.io/personal-blog/tokenization" rel="alternate" type="text/html" title="Tokenization for language modeling: Byte Pair Encoding vs Unigram Language Modeling" /><published>2020-07-09T00:00:00-05:00</published><updated>2020-07-09T00:00:00-05:00</updated><id>https://ndingwall.github.io/personal-blog/tokenization</id><author><name></name></author><category term="tokenization" /><category term="language models" /><category term="byte pair encoding" /><category term="unigram language model" /><summary type="html"><![CDATA[Tokenizers used by the best-performing language models (Bert, GPT-2, etc.) poorly reflect the morphology of English text. I had hoped to use some quarantine time to design one that more closely aligns to relationships between wordforms. But Kaj Bostrom and Greg Durrett beat me to it and so this blog post materialized instead. I add some additional motivation, evaluate both methods against 'gold standard' tokenizations, and speculate about what might come next.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ndingwall.github.io/personal-blog/images/tokenization-preview.png" /><media:content medium="image" url="https://ndingwall.github.io/personal-blog/images/tokenization-preview.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>